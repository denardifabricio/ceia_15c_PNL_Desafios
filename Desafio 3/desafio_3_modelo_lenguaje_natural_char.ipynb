{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Modelo de lenguaje con tokenización por caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consigna\n",
    "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
    "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
    "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
    "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
    "\n",
    "\n",
    "### Sugerencias\n",
    "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
    "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
    "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# Usaremos las utilidades de procesamiento de textos y secuencias de Keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # equivalente a ltokenizer de nltk\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence # equivalente a word_tokenize de nltk\n",
    "from tensorflow.keras.utils import pad_sequences # se utilizará para padding\n",
    "\n",
    "from keras.layers import Input, TimeDistributed, CategoryEncoding, SimpleRNN, Dense\n",
    "from keras.models import Model, Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 5440\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así como lo hicimos en el desafío anterior, buscamos las diferentes recetas del mismo. Para simplificar el desarrollo del TP, voy a replicar algunas funciones, aunque la idea en un entorno producitvo es tener locaciones comunes, librerías estandars de recupero y manipulación de la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipes_files(folder_paths):\n",
    "    files = []\n",
    "    for folder_path in folder_paths:\n",
    "        files.extend([os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.txt') or file.endswith('.md')])\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files:\n",
      "['../Desafio 2/recipes_1/almonds.txt', '../Desafio 2/recipes_1/fava.txt', '../Desafio 2/recipes_1/squares.txt', '../Desafio 2/recipes_1/recipes2.txt', '../Desafio 2/recipes_1/brownies.txt', '../Desafio 2/recipes_1/pannacotta.txt', '../Desafio 2/recipes_1/bruschetta.txt', '../Desafio 2/recipes_1/sweet_potato_pie.txt', '../Desafio 2/recipes_1/sorbet.txt', '../Desafio 2/recipes_1/cauli.txt', '../Desafio 2/recipes_1/tart.txt', '../Desafio 2/recipes_1/cornbread.txt', '../Desafio 2/recipes_2/crispy-beef-with-egg-fried-rice.md', '../Desafio 2/recipes_2/classic-duck-breast.md', '../Desafio 2/recipes_2/crispy-sesame-chicken.md', '../Desafio 2/recipes_2/tomato-pasta.md', '../Desafio 2/recipes_2/beef-stroganoff.md', '../Desafio 2/recipes_2/carrot-cake.md', '../Desafio 2/recipes_2/cacio-e-peppe.md', '../Desafio 2/recipes_2/pizza-sauce.md', '../Desafio 2/recipes_2/tagliatelle-with-broccoli-cauliflower-and-blue-cheese.md', '../Desafio 2/recipes_2/pizza-dogs.md', '../Desafio 2/recipes_2/meatballs.md', '../Desafio 2/recipes_2/chicken-balmoral.md', '../Desafio 2/recipes_2/chicken-broth-soup.md', '../Desafio 2/recipes_2/thai-green-curry.md', '../Desafio 2/recipes_2/blueberry-and-raspberry-muffins.md', '../Desafio 2/recipes_2/prawn-pad-thai.md', '../Desafio 2/recipes_2/toad-in-the-hole.md', '../Desafio 2/recipes_2/garlic-dough-balls.md', '../Desafio 2/recipes_2/school-fudge-tart.md', '../Desafio 2/recipes_2/haggis-tatties.md', '../Desafio 2/recipes_2/espresso-martini.md', '../Desafio 2/recipes_2/clover-club.md', '../Desafio 2/recipes_2/whiskey-sour.md', '../Desafio 2/recipes_2/mums-delicious-chocolate-brownies.md', '../Desafio 2/recipes_2/stuffed-bell-peppers.md', '../Desafio 2/recipes_2/fish-pie.md', '../Desafio 2/recipes_2/sticky-salmon-with-broccoli.md', '../Desafio 2/recipes_2/chicken-pasta-bake.md', '../Desafio 2/recipes_2/cauliflower-cheese.md', '../Desafio 2/recipes_2/crispy-honey-chicken.md', '../Desafio 2/recipes_2/tomato-garlic-and-prawn-pasta.md', '../Desafio 2/recipes_2/green-pea-chowder.md', '../Desafio 2/recipes_2/béchamel-sauce.md', '../Desafio 2/recipes_2/granddads-carrot-soup.md', '../Desafio 2/recipes_2/beef-and-broccoli-stir-fry.md', '../Desafio 2/recipes_2/sausage-and-bean-hotpot.md', '../Desafio 2/recipes_2/chicken-carbonara.md', '../Desafio 2/recipes_2/prawn-alfredo.md', '../Desafio 2/recipes_2/steak-pinwheels.md', '../Desafio 2/recipes_2/chicken-katsu-curry.md', '../Desafio 2/recipes_2/creamy-garlic-chicken.md', '../Desafio 2/recipes_2/roasted-pepper-pasta.md', '../Desafio 2/recipes_2/bbq-halloumi.md', '../Desafio 2/recipes_2/yorkshire-puddings.md', '../Desafio 2/recipes_2/bosnian-stuffed-peppers-punjene-paprike-.md', '../Desafio 2/recipes_2/warm-salmon-beetroot-freekeh-salad.md', '../Desafio 2/recipes_2/mediterranean-chicken-traybake.md', '../Desafio 2/recipes_2/chicken-schnitzel-.md', '../Desafio 2/recipes_2/grilled-steak-ratatouille-and-saffron-rice.md', '../Desafio 2/recipes_2/cinnamon-swirls.md', '../Desafio 2/recipes_2/chicken-piccata.md', '../Desafio 2/recipes_2/banana-milkshake.md', '../Desafio 2/recipes_2/pesto-tagliatelle-with-sea-bass.md', '../Desafio 2/recipes_2/lamb-rogan-josh.md', '../Desafio 2/recipes_2/tandoori-chicken.md', '../Desafio 2/recipes_2/charred-onion-whipped-feta-flatbreads.md', '../Desafio 2/recipes_2/halloumi-pasta-salad.md', '../Desafio 2/recipes_2/rocket-salad.md', '../Desafio 2/recipes_2/strawberry-and-banana-dessert-pizza-with-a-cookie-base.md', '../Desafio 2/recipes_2/smoked-haddock-risotto.md', '../Desafio 2/recipes_2/creamy-smoked-salmon-pasta.md', '../Desafio 2/recipes_2/chicken-gyoza.md', '../Desafio 2/recipes_2/chicken-kiev-pasta.md', '../Desafio 2/recipes_2/beef-and-guinness-stew.md', '../Desafio 2/recipes_2/margherita-pizza.md', '../Desafio 2/recipes_2/whisky-haggis-sauce.md', '../Desafio 2/recipes_2/bosnian-stuffed-peppers-punjene-paprike.md', '../Desafio 2/recipes_2/mash-potato.md']\n"
     ]
    }
   ],
   "source": [
    "folder_paths = ['../Desafio 2/recipes_1', '../Desafio 2/recipes_2']\n",
    "all_files = get_recipes_files(folder_paths)\n",
    "print(\"All files:\")\n",
    "print(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El encoding latin-1 es necesario para leer los archivos de texto. Si no se especifica, se produce un error de UnicodeDecodeError:\n",
    "# UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 2: invalid continuation byte\n",
    "df_recipes = pd.DataFrame({'recipe': [open(file, encoding='latin-1').read() for file in all_files]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de documentos: 80\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de documentos:\", df_recipes.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recipe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLANCHING NUTS: In the case of nuts, especiall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(fava beans)\\n\\ntreat gently when young and fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>These are kid tested, mother approved. From Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nless.\\n\\n1 cup dried split peas 1/2 teaspoon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fooder's Brownies (I usually 1/2 the recipe to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>---\\ntitle: Beef and Guinness stew\\ndate: 2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>---\\ntitle: Margherita pizza\\ndate: 2021-07-18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>---\\ntitle: Whisky Haggis sauce\\ndate: 2023-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>---\\ntitle: Bosnian Stuffed Peppers (Punjene P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>---\\ntitle: Mash potato\\ndate: 2021-08-02T13:4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               recipe\n",
       "0   BLANCHING NUTS: In the case of nuts, especiall...\n",
       "1   (fava beans)\\n\\ntreat gently when young and fr...\n",
       "2   These are kid tested, mother approved. From Co...\n",
       "3   \\nless.\\n\\n1 cup dried split peas 1/2 teaspoon...\n",
       "4   Fooder's Brownies (I usually 1/2 the recipe to...\n",
       "..                                                ...\n",
       "75  ---\\ntitle: Beef and Guinness stew\\ndate: 2021...\n",
       "76  ---\\ntitle: Margherita pizza\\ndate: 2021-07-18...\n",
       "77  ---\\ntitle: Whisky Haggis sauce\\ndate: 2023-01...\n",
       "78  ---\\ntitle: Bosnian Stuffed Peppers (Punjene P...\n",
       "79  ---\\ntitle: Mash potato\\ndate: 2021-08-02T13:4...\n",
       "\n",
       "[80 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLANCHING NUTS: In the case of nuts, especially with almonds, pistachios \n",
      "and hazelnuts, in addition to the tough outer husk, they have a thin inner \n",
      "lining which needs to be removed. This lining can be bitter and somewhat \n",
      "unattractive when found in confection and baked goods. Blanching can be a \n",
      "time consuming process. Think about  purchasing them already blanched and \n",
      "skinned. Nuts can then be chopped and then toasted, if desired.\n",
      "\n",
      "ALMONDS:\t\n",
      "Shell almonds and place them in a saucepan. Cover them with water.  \n",
      "\n",
      "Boil for 2 to 3 minutes.  Drain and rinse nuts in cold water.  \n",
      "\n",
      "Pinch off the almond skins by holding them at one end with your \n",
      "index finger and thumb, and then while pointing them into a bowl, press \n",
      "your fingers together -- be careful because they can shoot across the room!  \n",
      "\t\n",
      "Place blanched nuts on a jelly roll or rimmed pan and dry them out \n",
      "in a 300 degree preheated oven for about 5 - 10 minutes. Don't let them \n",
      "brown.\n",
      "\n",
      "----------\n",
      "\n",
      "                    *  Exported from \n"
     ]
    }
   ],
   "source": [
    "def concatenate_recipes(df):\n",
    "    \"\"\"\n",
    "    Concatena todas las recetas en un solo string.\n",
    "    \"\"\"\n",
    "    return \" \".join(df['recipe'].tolist())\n",
    "\n",
    "# Concatenar todas las recetas\n",
    "all_recipes_text = concatenate_recipes(df_recipes)\n",
    "print(all_recipes_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elegir el tamaño del contexto\n",
    "\n",
    "En este caso, como el modelo de lenguaje es por caracteres, todo un gran corpus\n",
    "de texto puede ser considerado un documento en sí mismo y el tamaño de contexto\n",
    "puede ser elegido con más libertad en comparación a un modelo de lenguaje tokenizado por palabras y dividido en documentos más acotados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleccionamos el tamaño de contexto\n",
    "max_context_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_vocab = set(all_recipes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# la longitud de vocabulario de caracteres es:\n",
    "len(chars_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos los dicionarios que asignan índices a caracteres y viceversa.\n",
    "# El diccionario `char2idx` servirá como tokenizador.\n",
    "char2idx = {k: v for v,k in enumerate(chars_vocab)}\n",
    "idx2char = {v: k for k,v in char2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_recipes_text = [char2idx[ch] for ch in all_recipes_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[89,\n",
       " 54,\n",
       " 20,\n",
       " 25,\n",
       " 51,\n",
       " 113,\n",
       " 98,\n",
       " 25,\n",
       " 73,\n",
       " 14,\n",
       " 25,\n",
       " 130,\n",
       " 88,\n",
       " 81,\n",
       " 27,\n",
       " 14,\n",
       " 98,\n",
       " 111,\n",
       " 14,\n",
       " 5,\n",
       " 65,\n",
       " 4,\n",
       " 14,\n",
       " 61,\n",
       " 80,\n",
       " 121,\n",
       " 4,\n",
       " 14,\n",
       " 126,\n",
       " 117,\n",
       " 14,\n",
       " 111,\n",
       " 86,\n",
       " 5,\n",
       " 121,\n",
       " 44,\n",
       " 14,\n",
       " 4,\n",
       " 121,\n",
       " 16,\n",
       " 4,\n",
       " 61,\n",
       " 82,\n",
       " 80,\n",
       " 119,\n",
       " 119,\n",
       " 47,\n",
       " 14,\n",
       " 15,\n",
       " 82,\n",
       " 5,\n",
       " 65,\n",
       " 14,\n",
       " 80,\n",
       " 119,\n",
       " 100,\n",
       " 126,\n",
       " 111,\n",
       " 60,\n",
       " 121,\n",
       " 44,\n",
       " 14,\n",
       " 16,\n",
       " 82,\n",
       " 121,\n",
       " 5,\n",
       " 80,\n",
       " 61,\n",
       " 65,\n",
       " 82,\n",
       " 126,\n",
       " 121,\n",
       " 14,\n",
       " 101,\n",
       " 80,\n",
       " 111,\n",
       " 60,\n",
       " 14,\n",
       " 65,\n",
       " 80,\n",
       " 104,\n",
       " 4,\n",
       " 119,\n",
       " 111,\n",
       " 86,\n",
       " 5,\n",
       " 121,\n",
       " 44,\n",
       " 14,\n",
       " 82,\n",
       " 111,\n",
       " 14,\n",
       " 80,\n",
       " 60,\n",
       " 60,\n",
       " 82,\n",
       " 5,\n",
       " 82,\n",
       " 126,\n",
       " 111]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_recipes_text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizando y estructurando el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separaremos el dataset entre entrenamiento y validación.\n",
    "# `p_val` será la proporción del corpus que se reservará para validación\n",
    "# `num_val` es la cantidad de secuencias de tamaño `max_context_size` que se usará en validación\n",
    "p_val = 0.2\n",
    "num_val = int(np.ceil(len(tokenized_recipes_text)*p_val/max_context_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separamos la porción de texto utilizada en entrenamiento de la de validación.\n",
    "train_text = tokenized_recipes_text[:-num_val*max_context_size]\n",
    "val_text = tokenized_recipes_text[-num_val*max_context_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences_val = [val_text[init*max_context_size:init*(max_context_size+1)] for init in range(num_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences_train = [train_text[init:init+max_context_size] for init in range(len(train_text)-max_context_size+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenemos el vector input y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(tokenized_sentences_train[:-1])\n",
    "y = np.array(tokenized_sentences_train[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a explorar un poco el input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178637, 100)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 89,  54,  20,  25,  51, 113,  98,  25,  73,  14])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del vocabulario es: 139\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars_vocab)\n",
    "\n",
    "print(f\"El tamaño del vocabulario es: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definir el Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo que se propone como ejemplo consume los índices de los tokens y los transforma en vectores OHE (en este caso no entrenamos una capa de embedding para caracteres). Esa transformación se logra combinando las capas `CategoryEncoding` que transforma a índices a vectores OHE y `TimeDistributed` que aplica la capa a lo largo de la dimensión \"temporal\" de la secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, None, 139)        0         \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, None, 200)         68000     \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 139)         27939     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 95,939\n",
      "Trainable params: 95,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(CategoryEncoding(num_tokens=vocab_size, output_mode = \"one_hot\"),input_shape=(None,1)))\n",
    "model.add(SimpleRNN(200, return_sequences=True, dropout=0.1, recurrent_dropout=0.1 ))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a construir la función de perplejidad mediante el uso de una función callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PplCallback(keras.callbacks.Callback):\n",
    "\n",
    "    '''\n",
    "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
    "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
    "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
    "    Además implementa la finalización del entrenamiento (Early Stopping)\n",
    "    si la perplejidad no mejora después de `patience` epochs.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, val_data, history_ppl,patience=5):\n",
    "      # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
    "      # mediremos la perplejidad\n",
    "      self.val_data = val_data\n",
    "\n",
    "      self.target = []\n",
    "      self.padded = []\n",
    "\n",
    "      count = 0\n",
    "      self.info = []\n",
    "      self.min_score = np.inf\n",
    "      self.patience_counter = 0\n",
    "      self.patience = patience\n",
    "\n",
    "      # nos movemos en todas las secuencias de los datos de validación\n",
    "      for seq in self.val_data:\n",
    "\n",
    "        len_seq = len(seq)\n",
    "        # armamos todas las subsecuencias\n",
    "        subseq = [seq[:i] for i in range(1,len_seq)]\n",
    "        self.target.extend([seq[i] for i in range(1,len_seq)])\n",
    "\n",
    "        if len(subseq)!=0:\n",
    "\n",
    "          self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
    "\n",
    "          self.info.append((count,count+len_seq))\n",
    "          count += len_seq\n",
    "\n",
    "      self.padded = np.vstack(self.padded)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        # en `scores` iremos guardando la perplejidad de cada secuencia\n",
    "        scores = []\n",
    "\n",
    "        predictions = self.model.predict(self.padded,verbose=0)\n",
    "\n",
    "        # para cada secuencia de validación\n",
    "        for start,end in self.info:\n",
    "\n",
    "          # en `probs` iremos guardando las probabilidades de los términos target\n",
    "          probs = [predictions[idx_seq,-1,idx_vocab] for idx_seq, idx_vocab in zip(range(start,end),self.target[start:end])]\n",
    "\n",
    "          # calculamos la perplejidad por medio de logaritmos\n",
    "          scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
    "\n",
    "        # promediamos todos los scores e imprimimos el valor promedio\n",
    "        current_score = np.mean(scores)\n",
    "        history_ppl.append(current_score)\n",
    "        print(f'\\n mean perplexity: {current_score} \\n')\n",
    "\n",
    "        # chequeamos si tenemos que detener el entrenamiento\n",
    "        if current_score < self.min_score:\n",
    "          self.min_score = current_score\n",
    "          self.model.save(\"my_model\")\n",
    "          print(\"Saved new model!\")\n",
    "          self.patience_counter = 0\n",
    "        else:\n",
    "          self.patience_counter += 1\n",
    "          if self.patience_counter == self.patience:\n",
    "            print(\"Stopping training...\")\n",
    "            self.model.stop_training = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256 if BATCH_SIZE is None else BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 2.5381\n",
      " mean perplexity: 7.804616251749798 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new model!\n",
      "698/698 [==============================] - 156s 223ms/step - loss: 2.5381\n",
      "Epoch 2/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 2.0512\n",
      " mean perplexity: 6.370157874886093 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new model!\n",
      "698/698 [==============================] - 222s 318ms/step - loss: 2.0512\n",
      "Epoch 3/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.8662\n",
      " mean perplexity: 5.495901226614644 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new model!\n",
      "698/698 [==============================] - 808s 1s/step - loss: 1.8662\n",
      "Epoch 4/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.7667\n",
      " mean perplexity: 5.066523050978434 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new model!\n",
      "698/698 [==============================] - 179s 257ms/step - loss: 1.7667\n",
      "Epoch 5/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.7072\n",
      " mean perplexity: 4.858426444205641 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new model!\n",
      "698/698 [==============================] - 190s 272ms/step - loss: 1.7072\n",
      "Epoch 6/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.6671\n",
      " mean perplexity: 4.718975515406043 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new model!\n",
      "698/698 [==============================] - 187s 269ms/step - loss: 1.6671\n",
      "Epoch 7/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.6381\n",
      " mean perplexity: 4.6652360245076885 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new model!\n",
      "698/698 [==============================] - 194s 277ms/step - loss: 1.6381\n",
      "Epoch 8/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.6163\n",
      " mean perplexity: 4.621526535364996 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new model!\n",
      "698/698 [==============================] - 200s 287ms/step - loss: 1.6163\n",
      "Epoch 9/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.5997\n",
      " mean perplexity: 4.549790946752569 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new model!\n",
      "698/698 [==============================] - 204s 293ms/step - loss: 1.5997\n",
      "Epoch 10/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.5851\n",
      " mean perplexity: 4.599713465411965 \n",
      "\n",
      "698/698 [==============================] - 181s 259ms/step - loss: 1.5851\n",
      "Epoch 11/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.5734\n",
      " mean perplexity: 4.4882247567222935 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new model!\n",
      "698/698 [==============================] - 174s 249ms/step - loss: 1.5734\n",
      "Epoch 12/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.5639\n",
      " mean perplexity: 4.477103224320683 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new model!\n",
      "698/698 [==============================] - 219s 314ms/step - loss: 1.5639\n",
      "Epoch 13/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.5548\n",
      " mean perplexity: 4.439423524320347 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new model!\n",
      "698/698 [==============================] - 212s 303ms/step - loss: 1.5548\n",
      "Epoch 14/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.5470\n",
      " mean perplexity: 4.508191742048347 \n",
      "\n",
      "698/698 [==============================] - 200s 287ms/step - loss: 1.5470\n",
      "Epoch 15/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.5405\n",
      " mean perplexity: 4.570510716296383 \n",
      "\n",
      "698/698 [==============================] - 196s 281ms/step - loss: 1.5405\n",
      "Epoch 16/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.5351\n",
      " mean perplexity: 4.529321752008171 \n",
      "\n",
      "698/698 [==============================] - 191s 274ms/step - loss: 1.5351\n",
      "Epoch 17/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.5295\n",
      " mean perplexity: 4.5790521657984 \n",
      "\n",
      "698/698 [==============================] - 184s 264ms/step - loss: 1.5295\n",
      "Epoch 18/20\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.5245\n",
      " mean perplexity: 4.504259900569355 \n",
      "\n",
      "Stopping training...\n",
      "698/698 [==============================] - 211s 303ms/step - loss: 1.5245\n"
     ]
    }
   ],
   "source": [
    "# fiteamos, nótese el agregado del callback con su inicialización. El batch_size lo podemos seleccionar a mano\n",
    "# en general, lo mejor es escoger el batch más grande posible que minimice el tiempo de cada época.\n",
    "# En la variable `history_ppl` se guardarán los valores de perplejidad para cada época.\n",
    "history_ppl = []\n",
    "hist = model.fit(X, y, epochs=20, callbacks=[PplCallback(tokenized_sentences_val,history_ppl)], batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Predicción del próximo caracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def model_response(human_text):\n",
    "\n",
    "    # Encodeamos\n",
    "    encoded = [char2idx[ch] for ch in human_text.lower() ]\n",
    "    # Si tienen distinto largo\n",
    "    encoded = pad_sequences([encoded], maxlen=max_context_size, padding='pre')\n",
    "\n",
    "    # Predicción softmax\n",
    "    y_hat = np.argmax(model.predict(encoded)[0,-1,:])\n",
    "\n",
    "\n",
    "    # Debemos buscar en el vocabulario el caracter\n",
    "    # que corresopnde al indice (y_hat) predicho por le modelo\n",
    "    out_word = ''\n",
    "    out_word = idx2char[y_hat]\n",
    "\n",
    "    # Agrego la palabra a la frase predicha\n",
    "    return human_text + out_word\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=model_response,\n",
    "    inputs=[\"textbox\"],\n",
    "    outputs=\"text\")\n",
    "\n",
    "iface.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se puede usar gradio para probar el modelo\n",
    "# Gradio es una herramienta muy útil para crear interfaces para ensayar modelos\n",
    "# https://gradio.app/\n",
    "\n",
    "!pip install -q gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generador de secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, seed_text, max_length, n_words):\n",
    "    \"\"\"\n",
    "        Exec model sequence prediction\n",
    "\n",
    "        Args:\n",
    "            model (keras): modelo entrenado\n",
    "            seed_text (string): texto de entrada (input_seq)\n",
    "            max_length (int): máxima longitud de la sequencia de entrada\n",
    "            n_words (int): números de caracteres a agregar a la sequencia de entrada\n",
    "        returns:\n",
    "            output_text (string): sentencia con las \"n_words\" agregadas\n",
    "    \"\"\"\n",
    "    output_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "\t\t# Encodeamos\n",
    "        encoded = [char2idx[ch] for ch in output_text.lower() ]\n",
    "\t\t# Si tienen distinto largo\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\n",
    "\t\t# Predicción softmax\n",
    "        y_hat = np.argmax(model.predict(encoded,verbose=0)[0,-1,:])\n",
    "\t\t# Vamos concatenando las predicciones\n",
    "        out_word = ''\n",
    "\n",
    "        out_word = idx2char[y_hat]\n",
    "\n",
    "\t\t# Agrego las palabras a la frase predicha\n",
    "        output_text += out_word\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add salt to the pasta into the pasta into the'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text='add salt to the'\n",
    "\n",
    "generate_seq(model, input_text, max_length=max_context_size, n_words=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Beam search y muestreo aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcionalidades para hacer encoding y decoding\n",
    "\n",
    "def encode(text,max_length=max_context_size):\n",
    "\n",
    "    encoded = [char2idx[ch] for ch in text]\n",
    "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def decode(seq):\n",
    "    return ''.join([idx2char[ch] for ch in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "# función que selecciona candidatos para el beam search\n",
    "def select_candidates(pred,num_beams,vocab_size,history_probs,history_tokens,temp,mode):\n",
    "\n",
    "  # colectar todas las probabilidades para la siguiente búsqueda\n",
    "  pred_large = []\n",
    "\n",
    "  for idx,pp in enumerate(pred):\n",
    "    pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
    "\n",
    "  pred_large = np.array(pred_large)\n",
    "\n",
    "  # criterio de selección\n",
    "  if mode == 'det':\n",
    "    idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
    "  elif mode == 'sto':\n",
    "    idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams, p=softmax(pred_large/temp)) # beam search con muestreo aleatorio\n",
    "  else:\n",
    "    raise ValueError(f'Wrong selection mode. {mode} was given. det and sto are supported.')\n",
    "\n",
    "  # traducir a índices de token en el vocabulario\n",
    "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
    "                        np.array([idx_select%vocab_size]).T),\n",
    "                      axis=1)\n",
    "\n",
    "  # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
    "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
    "\n",
    "\n",
    "def beam_search(model,num_beams,num_words,input,temp=1,mode='det'):\n",
    "\n",
    "    # first iteration\n",
    "\n",
    "    # encode\n",
    "    encoded = encode(input)\n",
    "\n",
    "    # first prediction\n",
    "    y_hat = model.predict(encoded,verbose=0)[0,-1,:]\n",
    "\n",
    "    # get vocabulary size\n",
    "    vocab_size = y_hat.shape[0]\n",
    "\n",
    "    # initialize history\n",
    "    history_probs = [0]*num_beams\n",
    "    history_tokens = [encoded[0]]*num_beams\n",
    "\n",
    "    # select num_beams candidates\n",
    "    history_probs, history_tokens = select_candidates([y_hat],\n",
    "                                        num_beams,\n",
    "                                        vocab_size,\n",
    "                                        history_probs,\n",
    "                                        history_tokens,\n",
    "                                        temp,\n",
    "                                        mode)\n",
    "\n",
    "    # beam search loop\n",
    "    for i in range(num_words-1):\n",
    "\n",
    "      preds = []\n",
    "\n",
    "      for hist in history_tokens:\n",
    "\n",
    "        # actualizar secuencia de tokens\n",
    "        input_update = np.array([hist[i+1:]]).copy()\n",
    "\n",
    "        # predicción\n",
    "        y_hat = model.predict(input_update,verbose=0)[0,-1,:]\n",
    "\n",
    "        preds.append(y_hat)\n",
    "\n",
    "      history_probs, history_tokens = select_candidates(preds,\n",
    "                                                        num_beams,\n",
    "                                                        vocab_size,\n",
    "                                                        history_probs,\n",
    "                                                        history_tokens,\n",
    "                                                        temp,\n",
    "                                                        mode)\n",
    "\n",
    "    return history_tokens[:,-(len(input)+num_words):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: add salt to the\n",
      "Ouput:add salt to the oven \n",
      "----------------------------------\n",
      "Input: put the chicken in the\n",
      "Ouput:put the chicken in the oven \n",
      "----------------------------------\n",
      "Input: heat the oil in a\n",
      "Ouput:heat the oil in a large\n",
      "----------------------------------\n",
      "Input: add the flour to the\n",
      "Ouput:add the flour to the sauce\n",
      "----------------------------------\n",
      "Input: mix the eggs with the\n",
      "Ouput:mix the eggs with the sauce\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "inputs = [\"add salt to the\",\"put the chicken in the\", \"heat the oil in a\", \"add the flour to the\", \"mix the eggs with the\"]\n",
    "\n",
    "for input in inputs:\n",
    "  print(f'Input: {input}')\n",
    "  salida = beam_search(model,num_beams=10,num_words=6,input=input,temp=1,mode='sto')\n",
    "  print(f'Ouput:{decode(salida[0])}')\n",
    "\n",
    "  print ('----------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones finales\n",
    "1. Encontré que el costo computacional del entrenamiento del modelo es muy alto. Considerando que limité bastante el número de recetas, en un ambiente productivo, no lo veo óptimo para su uso. Esto se repite tanto como en el modelo de word como este, aunque aquí, al procesar por caracteres, se refuerza esta problemática.\n",
    "\n",
    "2. En cuanto a la predicción de secuencias, noto que las primeras palabras tienen sentido, no así si intentamos predecir una cadena más larga, en donde, a pesar de encontrar palabras del idioma inglés (las recetas están escritas en dicho idioma), se va perdiendo el sentido de la oración, por ej: \"add salt to the pasta into the pasta into the\".\n",
    "\n",
    "3. Utilizando beam search y muestreo aleatorio, y a pesar de la simplesa del algoritmo y que limité el número de recetas, encuentro que las secuencia sugerencias tienen un léxico coherente, en cuanto a la semántica, tal vez falla en algunas, por ejemplo, uno no agrega sal dentro del horno, pero sí se lo hace con un pollo (predicción correcta encontrada por el modelo también)\n",
    "\n",
    "4. Comparando el modelo por palabras y por caracter, considero que, aunque no con grandes diferencias, el modelo por caracter tienen mejor accuracy, analizándolo semánticamente, es decir revisando si la salida de las predicciones tienen sentido en el contexto de las recetas de cocina.\n",
    "\n",
    "## Próximos pasos\n",
    "Al igual que mencioné en el modelo por palabras, para realizar un mejor análisis, se podría usar Optuna u algún otro framework de evaluación de hiperparámetros, para ir \"jugando\" con los diferentes hiperparámetros, por una cuestión de tiempo de procesamiento y costo de GPU, no lo implementé, pero sí sería necesario en caso que en mi TP final quiera aplicar algo de esta temática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fabricio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
